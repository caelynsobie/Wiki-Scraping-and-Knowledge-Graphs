{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4db2b0dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wikipediaapi\n",
    "import pandas as pd \n",
    "import concurrent.futures\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3480e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wiki_page(page_name):\n",
    "    wiki_api = wikipediaapi.Wikipedia('Graph Theory Knowledge graphs (caelynsobie@gmail.com)', language='en',\n",
    "              extract_format=wikipediaapi.ExtractFormat.WIKI)\n",
    "    page_name = wiki_api.page(page_name)\n",
    "    if not page_name.exists():\n",
    "        print('Page {} does not exist.'.format(page_name))\n",
    "        return\n",
    "    \n",
    "    page_data = pd.DataFrame({\n",
    "        'page': page_name,\n",
    "        'text': page_name.text,\n",
    "        'link': page_name.fullurl,\n",
    "        'categories': [[y[9:] for y in \n",
    "                       list(page_name.categories.keys())]],\n",
    "    })\n",
    "    \n",
    "    return page_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f1a6e377",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "\n",
    "def get_entity_pairs(text, coref=True):\n",
    "    # preprocess text\n",
    "    text = re.sub(r'\\n+', '.', text)  # replace multiple newlines with period\n",
    "    text = re.sub(r'\\[\\d+\\]', ' ', text)  # remove reference numbers\n",
    "    text = nlp(text)\n",
    "\n",
    "    def refine_ent(ent, sent):\n",
    "        unwanted_tokens = (\n",
    "            'PRON',  # pronouns\n",
    "            'PART',  # particle\n",
    "            'DET',  # determiner\n",
    "            'SCONJ',  # subordinating conjunction\n",
    "            'PUNCT',  # punctuation\n",
    "            'SYM',  # symbol\n",
    "            'X',  # other\n",
    "        )\n",
    "        ent_type = ent.ent_type_  # get entity type\n",
    "        if ent_type == '':\n",
    "            ent_type = 'NOUN_CHUNK'\n",
    "            ent = ' '.join(str(t.text) for t in\n",
    "                           nlp(str(ent)) if t.pos_\n",
    "                           not in unwanted_tokens and t.is_stop == False)\n",
    "        elif ent_type in ('NOMINAL', 'CARDINAL', 'ORDINAL') and str(ent).find(' ') == -1:\n",
    "            refined = ''\n",
    "            for i in range(len(sent) - ent.i):\n",
    "                if ent.nbor(i).pos_ not in ('VERB', 'PUNCT'):\n",
    "                    refined += ' ' + str(ent.nbor(i))\n",
    "                else:\n",
    "                    ent = refined.strip()\n",
    "                    break\n",
    "\n",
    "        return ent, ent_type\n",
    "\n",
    "    sentences = [sent.text.strip() for sent in text.sents]  # split text into sentences\n",
    "    ent_pairs = []\n",
    "    for sent in sentences:\n",
    "        sent = nlp(sent)\n",
    "        spans = list(sent.ents) + list(sent.noun_chunks)  # collect nodes\n",
    "        spans = spacy.util.filter_spans(spans)\n",
    "        with sent.retokenize() as retokenizer:\n",
    "            [retokenizer.merge(span, attrs={'tag': span.root.tag,\n",
    "                                            'dep': span.root.dep}) for span in spans]\n",
    "        deps = [token.dep_ for token in sent]\n",
    "\n",
    "        # limit our example to simple sentences with one subject and object\n",
    "        if (deps.count('obj') + deps.count('dobj')) != 1\\\n",
    "                or (deps.count('subj') + deps.count('nsubj')) != 1:\n",
    "            continue\n",
    "\n",
    "        for token in sent:\n",
    "            if token.dep_ not in ('obj', 'dobj'):  # identify object nodes\n",
    "                continue\n",
    "            subject = [w for w in token.head.lefts if w.dep_\n",
    "                       in ('subj', 'nsubj')]  # identify subject nodes\n",
    "            if subject:\n",
    "                subject = subject[0]\n",
    "                # identify relationship by root dependency\n",
    "                relation = [w for w in token.ancestors if w.dep_ == 'ROOT']\n",
    "                if relation:\n",
    "                    relation = relation[0]\n",
    "                    # add adposition or particle to relationship\n",
    "                    if relation.nbor(1).pos_ in ('ADP', 'PART'):\n",
    "                        relation = ' '.join((str(relation), str(relation.nbor(1))))\n",
    "                else:\n",
    "                    relation = 'unknown'\n",
    "\n",
    "                subject, subject_type = refine_ent(subject, sent)\n",
    "                token, object_type = refine_ent(token, sent)\n",
    "\n",
    "                ent_pairs.append([str(subject), str(relation), str(token),\n",
    "                                  str(subject_type), str(object_type)])\n",
    "\n",
    "    ent_pairs = [sublist for sublist in ent_pairs\n",
    "                          if not any(str(ent) == '' for ent in sublist)]\n",
    "    pairs = pd.DataFrame(ent_pairs, columns=['subject', 'relation', 'object',\n",
    "                                             'subject_type', 'object_type'])\n",
    "    print('Entity pairs extracted:', str(len(ent_pairs)))\n",
    "    print(pairs)\n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7963c8f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create knowledge graph\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def draw_kg(pairs):\n",
    "    k_graph = nx.from_pandas_edgelist(pairs, 'subject', 'object',\n",
    "            create_using=nx.MultiDiGraph())\n",
    "    node_deg = nx.degree(k_graph)\n",
    "    layout = nx.spring_layout(k_graph, k=0.15, iterations=20)\n",
    "    plt.figure(num=None, figsize=(16, 12), dpi=80)\n",
    "    nx.draw_networkx(\n",
    "        k_graph,\n",
    "        node_size=[int(deg[1]) * 500 for deg in node_deg],\n",
    "        arrowsize=20,\n",
    "        linewidths=1.5,\n",
    "        pos=layout,\n",
    "        edge_color='red',\n",
    "        edgecolors='black',\n",
    "        node_color='white',\n",
    "        )\n",
    "    labels = dict(zip(list(zip(pairs.subject, pairs.object)),\n",
    "                  pairs['relation'].tolist()))\n",
    "    nx.draw_networkx_edge_labels(k_graph, pos=layout, edge_labels=labels,\n",
    "                                 font_color='red')\n",
    "    plt.axis('off')\n",
    "    plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b84fb51f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create filtered knowledge graph\n",
    "def filter_graph(pairs, node):\n",
    "    k_graph = nx.from_pandas_edgelist(pairs, 'subject', 'object',\n",
    "            create_using=nx.MultiDiGraph())\n",
    "    edges = nx.dfs_successors(k_graph, node)\n",
    "    nodes = []\n",
    "    for k, v in edges.items():\n",
    "        nodes.extend([k])\n",
    "        nodes.extend(v)\n",
    "    subgraph = k_graph.subgraph(nodes)\n",
    "    layout = (nx.planar_layout(k_graph))\n",
    "    plt.figure(num=None, figsize=(16, 12), dpi=80)\n",
    "    nx.draw_networkx(\n",
    "        subgraph,\n",
    "        node_size=1000,\n",
    "        arrowsize=20,\n",
    "        linewidths=1.5,\n",
    "        pos=layout,\n",
    "        edge_color='red',\n",
    "        edgecolors='black',\n",
    "        node_color='white'\n",
    "        )\n",
    "    labels = dict(zip((list(zip(pairs.subject, pairs.object))),\n",
    "                    pairs['relation'].tolist()))\n",
    "    edges= tuple(subgraph.out_edges(data=False))\n",
    "    sublabels ={k: labels[k] for k in edges}\n",
    "    nx.draw_networkx_edge_labels(subgraph, pos=layout, edge_labels=sublabels,\n",
    "                                font_color='red')\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
